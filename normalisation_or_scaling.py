# -*- coding: utf-8 -*-
"""Normalisation or scaling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h3bw_n7NE_rtczNswLogcAyMMgxPU8Ob
"""

# 1. Before Normalisation
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(0) # To keep the randomly generated values constant
x1 = np.random.randint(1,50,30)
x1 = np.sort(x1) # It will sort in ascending order
x2 = np.random.randint(10000, 70000, 30)
plt.plot(x1,x2)

# 2. After Normalisation
x1min = min(x1)
x1max = max(x1)
x2min = min(x2)
x2max = max(x2)
x1norm = (x1-x1min)/(x1max-x1min)
x2norm = (x2-x2min)/(x2max-x2min)
plt.plot(x1norm, x2norm)

# CLASSIFICATION - LOGISTIC REGRESSION
# It classifies our data (either 0 or 1, true or false, Person has brain tumor or not, car or bike, cat or dog, tea or coffee, 1 or 2 or 3)

# NETWORK ADS DATASET FROM GITHUB @ AMEENMANNA8824
# https://raw.githubusercontent.com/ameenmanna8824/DATASETS/main/Social_Network_Ads.csv
import pandas as pd
df = pd.read_csv('https://raw.githubusercontent.com/ameenmanna8824/DATASETS/main/Social_Network_Ads.csv')
df

# purchased column has only 0's and 1's
# 0 - not purchased
# 1 - purchased
# output - purchased
# Input - Age and Estimated Salary

# 2. I just want to know how many people have purchased and how many people have not purchased
df['Purchased'].value_counts()

# 3. Is not required
# Input is 2 dimensional
# 4. Divide the data into input and output
x = df.iloc[:,2:4].values

x

# output is 1 dimensional
y = df.iloc[:,4].values

# 5. Splitting into train and test datasets
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,random_state=0)
# The variable name can be changed but the variable order cannot be changed
# input train should be 1st and input test 2nd and output train 3rd and output test 4th

print(x.shape)
print(x_train.shape)
print(x_test.shape)

print(y.shape)
print(y_train.shape)
print(y_test.shape)

# 6. Normalisation or Scaling (Done only for inputs)
# To identify - if the input has 
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.fit_transform(x_test)

x_train

# 7. apply a classifier or regressor or clusstoror
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()

# 8. Fitting the model
# giving the data to the logistic regression training
model.fit(x_train, y_train)

# 9. Predict the output 
y_pred = model.predict(x_test) # using the input testing values we predict the ouput
y_pred # Predicted Values

y_test # Actual values

# 10. To calculate accuracy
from sklearn.metrics import accuracy_score
accuracy_score(y_pred, y_test)*100

# Individual predictions
a = scaler.transform([[19, 19000]]) # here scaling or normailzation takes place
a

model.predict(a)

b = scaler.transform([[15, 35000]])
b

model.predict(b)

